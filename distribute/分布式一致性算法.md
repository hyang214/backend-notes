# 分布式一致性算法

Raft、Zab、VR、Gossip、Paxos


## 1. Raft: 

### 1.1 构成
+ 一个Leader: 处理所有客户端交互，日志复制等
+ 若干Follower: 接受leader传入的更新，对外提供查询服务

### 1.2 接受写请求(日志复制)
+ Leader接受client请求，将该请求转化成entry，然后添加到自己的log中，得到该entry的index信息。entry中就包含了当前leader的term信息和在log中的index信息
+ follwer收到请求后
	+ 重置HeartbeatTimeout，防止节点转变成candidate
	+ 检查传过来的请求term和当前follower的term，term不一致，表示出现过leader重新选举，原leader已经不能领导了，返回fasle
	+ 检查prevLogIndex和prevLogTerm和当前follower的对应index的log是否一致，不一致，表示出现了消息跳跃，该server有些消息没有消费，返回fasle
		+ leader根据返回的信息，重置对应follower的消息队列，从其上次消费之后的那个index开始重新发送
	+ 检查都通过后，follower就开始将entries中的数据全部覆盖到本地对应的index上，如果没有则算是添加如果有则算是更新，也就是说和leader的保持一致
	+ 最后follower将最后复制的index发给leader,同时返回ok，leader会像上述一样来更新follower的macthIndex
+ leader一旦发现有些entries已经被过半的follower复制了，则就将该entry提交，将commitIndex提升至该entry的index。（这里是按照entry的index先后顺序提交的），具体的实现可以通过follower发送过来macthIndex来判定是否过半了，一旦可以提交了，leader就将该entry应用到状态机中，然后给客户端回复OK
+ 然后在下一次heartBeat心跳中，将commitIndex就传给了所有的follower，对应的follower就可以将commitIndex以及之前的entry应用到各自的状态机中了

### 1.3 选举
+ 刚开始所有server启动都是follower状态，然后等待leader或者candidate的RPC请求、或者超时
	+ leader的AppendEntries RPC请求：更新term和leader信息，当前follower再重新重置到follower状态
	+ candidate的RequestVote RPC请求：为candidate进行投票，如果candidate的term比自己的大，则当前follower再重新重置到follower状态
	+ 超时：转变为candidate，开始发起选举投票
		+ 每个节点的 选举超时时间都是随机的，不一致(150ms-300ms)，这样就能更快的选举出新的leader
+ 转变为candidate后，向全部节点，发送reqeuest vote rpc
	+ 一个节点收到投票请求后
		+ 如果没有投票，那么对比candidate的log和当前server的log哪个更新，比较方式为谁的lastLog的term越大谁越新，如果term相同，谁的lastLog的index越大谁越新，最后投给最新的那个
		+ 如果已经投票了，返回null
+ 当原Leader或Candidate发现自己的Term比别的Follower小时Leader或Candidate将转为Follower

### 1.4 分区
过半即可，奇数个节点（2n+1），必须有个分区有n+1个节点才能对外提供服务；

### 1.5 Reference
[Raft算法赏析](https://my.oschina.net/pingpangkuangmo/blog/776714?spm=a2c4e.11153940.blogcont62901.10.3b002ee5tI0VuN)

## 2. Zab: Zookeeper automic broadcast protocol

### 2.1 构成
+ 一个Leader：负责处理外部客户端的事物请求(或写操作)，然后leader服务器将客户端的写操作数据同步到所有的follower节点中
+ 若干Follower: 接受leader传入的更新，对外提供查询服务
+ 若干Observer: 只接收同步数据，对外提供查询，但是不参与选举和被选举

### 2.2 接受写请求（广播模式）
+ Leader将客户端的写操作转化为事物(或提议proposal)
+ Leader节点在数据写完之后，将向所有的follower节点发送数据广播请求(或数据复制)，等待所有的follower节点反馈
+ 只要超过半数follower节点反馈OK，Leader节点就会向所有的follower服务器发送commit消息。即将leader节点上的数据同步到follower节点之上
	+ leader和follower之间通过tcp连接简历的FIFO通道进行传播
	+ 只有发送了commit消息成功就当做成功，而不需要等待全部节点都提交成功
+ 返回成功给client

#### 2.2.1 具体步骤
zookeeper中消息广播的具体步骤如下： 
+ 客户端发起一个写操作请求，只有leader节点可以接受写请求，follower收到请求后，转发给leader进行处理
+ Leader服务器将客户端的request请求转化为事物proposql提案，同时为每个proposal分配一个全局唯一的ID，即ZXID。
	+ ZXID共有64位
		+ 其中高32位为主leader本次作为leader的序号，下一个leader自动+1
		+ 其中低32位为本次leader主持的处理的内部序号，从0开始
		+ 这样保证重新选举后，或者正常处理过程中，序号都是单调增的
+ leader服务器与每个follower之间都有一个队列，leader将消息发送到该队列；
+ follower机器从队列中取出消息处理完(写入本地事物日志中)毕后，向leader服务器发送ACK确认；
+ leader服务器收到半数以上的follower的ACK后，即认为可以发送commit；
+ leader向所有的follower服务器发送commit消息；
	+ commit没收到怎么办？

### 2.3 从新选举主节点（恢复模式）
+ 如果leader服务器发生崩溃，则zab协议要求zookeeper集群进行崩溃恢复和leader服务器选举。
	+ Leader和Follower之间通过心跳判别健康状态，正常情况下Zab处在broadcast阶段
	+ leader崩溃
		+ 当服务框架在启动过程中
      	+ 当Leader服务器出现网络中断，崩溃退出与重启等异常情况。
      	+ 当集群中已经不存在过半的服务器与Leader服务器保持正常通信（网络隔离）
	+ Zab协议崩溃恢复要求满足如下2个要求： 
		+ 确保已经被leader提交的proposal必须最终被所有的follower服务器提交。 
			+ 在选择新的leader的时候，需要选择拥有 proposal 最大值（即 zxid 最大） 的节点作为新的 leader
				+ 如果有多个节点，那么进行选举，使用electronEpoch来标记选举代数，通过迭代，选举出获得半数实例投票的leader
					+ 其实是选择了zxid做的节点中SID最大的那个作为leader
		+ 确保丢弃已经被leader出的但是没有被提交的proposal。
	+ 新选举出来的leader不能包含未提交的proposal，即新选举的leader必须都是已经提交了的proposal的follower服务器节点。同时，新选举的leader节点中含有最高的ZXID。这样做的好处就是可以避免了leader服务器检查proposal的提交和丢弃工作。
		+ leader服务器发生崩溃时分为如下场景： 
			+ leader在提出proposal时未提交之前崩溃，则经过崩溃恢复之后，新选举的leader一定不能是刚才的leader。因为这个leader存在未提交的proposal。 
			+ leader在发送commit消息之后，崩溃。即消息已经发送到队列中。经过崩溃恢复之后，参与选举的follower服务器(刚才崩溃的leader有可能已经恢复运行，也属于follower节点范畴)中有的节点已经是消费了队列中所有的commit消息。即该follower节点将会被选举为最新的leader。剩下动作就是数据同步过程。

### 2.4 数据同步
+ 在zookeeper集群中新的leader选举成功之后，leader会将自身的提交的最大proposal的事物ZXID发送给其他的follower节点。
+ follower节点会根据leader的消息进行回退或者是数据同步操作。最终目的要保证集群中所有节点的数据副本保持一致。

### 2.5 分区
过半即可，奇数个节点（2n+1），必须有个分区有n+1个节点才能对外提供服务；

## 3. VR: Viewstamped Replication

### 3.1 构成
+ 一个primary: 规定所有数据操作，无论是查询还是更新都必须通过primary进行
+ 若干backup: 提供服务，接受primary同步的数据

### 3.2 数据写请求
+ 客户端C发送数据到primary
+ primary进行事务的prepare处理
	+ 所有更新（event）都发生在一个特定的view下(类似于raft的term和zab的zxid的前32位)
	+ event必须按照时间戳顺序执行
+ 客户端C进行事务提交
+ primary将数据同步给backup，半数以上cohor(primary+backup)响应，那么表示成功，返回给客户端

### 3.3 选举新primary(view change)
+ 当某个cohort通过心跳发现primary挂了后，发起view change，同时可能有多个cohort发起view change
+ 发起view change的节点
	+ 构造一个新的viewid，这个view id要比其所见到的所有的view id都大，并且不同的cohort生的view id一定不能相同，这点与paxos的proposalid非常像
	+ 发送一个invitation到其他活着的cohort
	+ 收到invitation的节点
		+ invitation中的viewId比见到的viewId都大，那么接受请求
		+ 否则，拒绝
	+ 节点收到了多数派的认可，那么新的viewId成功
+ 寻找新的primary，并把各cohort回应的viewstamp发送给primary初始化其状态，如果老的primary没有crash，可以继续指定为primary；否则随机指定cohort作为primary
+ 新Primary把在初始化时，把所有正确的viewstamp发送到所有的cohort，使所有的cohort状态一致

### 3.4 分区
接受一半不到的节点同时失效

### Reference
[viewstamp replication](https://blog.csdn.net/ggxxkkll/article/details/7872698)

## 4. Gossip

### 节点构成
+ 全部节点都是对等的，没有leader

### 基础概览
#### 通讯类型
对于两个节点（A、B）之间存在三种通信方式:
+ push: A节点将数据(key,value,version)及对应的版本号推送给B节点，B节点更新A中比自己新的数据
+ pull：A仅将数据key,version推送给B，B将本地比A新的数据（Key,value,version）推送给A，A更新本地
+ push/pull：与pull类似，只是多了一步，A再将本地比B新的数据推送给B，B更新本地

#### 工作模式
+ Anti-Entropy（反熵）：以固定的概率传播所有的数据
+ Rumor-Mongering（谣言传播）：仅传播新到达的数据


## 5. Paxos

### 5.1 节点构成
每个节点可以身兼多个角色
+ Client: 议题产生者
+ Proposer: 提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准
+ Acceptor: 提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值
+ Learner: 不参与提案和批准，只是学习批准后的结果，用于提高性能


### 写数据(提案)
分成2个阶段，第一阶段是询问提案是否能被接受，第二阶段是要求各个accepter接受提案
#### Phase 1
+ Client向Proposer发起提案请求，Proposer为提案生成一个编号，然后将该提案发送给全部acceptors，发起prepare消息
	+ 每个提案都有一个全局唯一的编号
		+ 
+ 每个acceptor收到请求后，按照如下条件进行处理
	+ 如果收到的提案的编号n，大于上次通过的编号（minProposal），那么
		+ 更新自身通过的最大编号 minProposal = n
		+ 返回成功
	+ 否则，返回不成功，并且返回上次通过的最大编号minProposal，以及对应的value
		+ 返回更高的编号的内容，让proposer提议该提案，这样可以更快达到一致
		+ 可能出现2个proposer一直不断提交提案，然后相互打断，出现了活锁
			+ proposer进行限制，选举主proposer
			+ proposer提案有时间延长，如果被覆盖的情况下，延长一段时间再进行提案
			+ 提案编号，生成逻辑不是固定步长
+ Proposer对于收到的回复
	+ 如果返回了比Proposer本身提交的n更大的编号，那么本次提案的编号和value都替换为更高的那个
	+ 如果收到了来着过半数的accepter的返回，那么说明提案通过
#### Phase 2
+ 提案已经得到了半数的accepter的认可，Proposer对最后生成的提案进行广播(n, value)
+ 对于accepter
	+ 如果收到的提案的编号n，大于上次通过的编号（minProposal），那么本地更新，然后返回
	+ 否则，返回失败，已经更高的编号与value
+ Proposer收到回复后
	+ 如果收到了更高的编号n，转回phase 1，重新prepare
	+ 如果有超过半数的accepter接受，那么表示value达成一致，该提案完成
#### learner学习
对于learner如果进行学习，有如下方案
+ 每个learner和全部accepter连接，然后任何accepter通过的提案，都进行学习
	+ 大量连接
+ 单个主learner，主learner和全部accepter连接，然后主learner和全部从learner连接
	+ 单点问题
+ 主learner群，多个主learner和全部accepter连接，剩下follower的learner和多个主learner连接

### Reference
[Paxos协议超级详细解释+简单实例](https://blog.csdn.net/cnh294141800/article/details/53768464/)
[paxos算法学习与推导](https://www.liangzl.com/get-article-detail-10316.html)














